== LogClass ==

This repository provides an open-source toolkit for LogClass framework from Meng, Weibin, et al. &quot;[https://ieeexplore.ieee.org/abstract/document/8624141 Device-agnostic log anomaly classification with partial labels].&quot; ''2018 IEEE/ACM 26th International Symposium on Quality of Service (IWQoS)''. IEEE, 2018. . 

LogClass combines a word representation method and the [https://github.com/aldro61/pu-learning PU learning model] to construct a device-agnostic vocabulary based on partial labels. 

=== Table of Contents ===

__TOC__



=== Requirements ===

Requirements are listed in <code>requirements.txt</code>. To install these, run:

<pre class="">pip install -r requirements.txt</pre>


=== Quick Start ===

==== Run LogClass ====

Several example experiments using LogClass are included in this repository. 

Here is an example to run one of them - training of the global experiment doing anomaly detection and classification. Run the following command in the home directory of this project: 

<pre class="">python -m LogClass.logclass --train --logs_type &quot;bgl&quot; --raw_logs &quot;./Data/RAS_LOGS&quot; --report macro</pre>


==== Arguments ====

<pre class="">python -m LogClass.logclass --help
usage: logclass.py [-h] [--raw_logs raw_logs] [--base_dir base_dir]
                   [--logs logs] [--models_dir models_dir]
                   [--features_dir features_dir] [--logs_type logs_type]
                   [--kfold kfold] [--healthy_label healthy_label]
                   [--features features [features ...]]
                   [--report report [report ...]]
                   [--binary_classifier binary_classifier]
                   [--multi_classifier multi_classifier] [--train] [--force]
                   [--id id] [--swap]

Runs binary classification with PULearning to detect anomalous logs.

optional arguments:
  -h, --help            show this help message and exit
  --raw_logs raw_logs   input raw logs file path (default: None)
  --base_dir base_dir   base output directory for pipeline output files
                        (default: ['{your_logclass_dir}\\output'])
  --logs logs           input logs file path and output for raw logs
                        preprocessing (default: None)
  --models_dir models_dir
                        trained models input/output directory path (default:
                        None)
  --features_dir features_dir
                        trained features_dir input/output directory path
                        (default: None)
  --logs_type logs_type
                        Input type of logs. (default: ['original'])
  --kfold kfold         kfold crossvalidation (default: None)
  --healthy_label healthy_label
                        the labels of unlabeled logs (default: ['unlabeled'])
  --features features [features ...]
                        Features to be extracted from the logs messages.
                        (default: ['tfilf'])
  --report report [report ...]
                        Reports to be generated from the model and its
                        predictions. (default: None)
  --binary_classifier binary_classifier
                        Binary classifier to be used as anomaly detector.
                        (default: ['pu_learning'])
  --multi_classifier multi_classifier
                        Multi-clas classifier to classify anomalies. (default:
                        ['svm'])
  --train               If set, logclass will train on the given data.
                        Otherwiseit will run inference on it. (default: False)
  --force               Force training overwriting previous output with same
                        id. (default: False)
  --id id               Experiment id. Automatically generated if not
                        specified. (default: None)
  --swap                Swap testing/training data in kfold cross validation.
                        (default: False)</pre>


==== Directory Structure ====

<pre class="">.
├── Data
│   └── open_source_logs				# Included open-source log datasets
│       ├── Apache
│       ├── bgl
│       ├── hadoop
│       ├── hdfs
│       ├── hpc
│       ├── proxifier
│       └── zookeeper
├── output								# Example output folder
│   ├── preprocessed_logs				# Saved preprocessed logs for reuse
│   │   ├── open_Apache.txt
│   │   └── open_bgl.txt
│   └── train_multi_open_bgl_2283696426	# Example experiment output
│       ├── best_params.json
│       ├── features
│       │   ├── tfidf.pkl
│       │   └── vocab.pkl
│       ├── models
│       │   └── multi.pkl
│       └── results.csv
├── feature_engineering
│   ├── __init__.py
│   ├── length.py
│   ├── tf_idf.py
│   ├── tf_ilf.py
│   ├── tf.py
│   ├── registry.py
│   ├── vectorizer.py					# Log message vectorizing utilities
│   └── utils.py
├── models
│   ├── __init__.py
│   ├── base_model.py					# BaseModel class extended by all models
│   ├── pu_learning.py
│   ├── regular.py
│   ├── svm.py
│   ├── binary_registry.py
│   └── multi_registry.py
├── preprocess
│   ├── __init__.py
│   ├── bgl_preprocessor.py
│   ├── open_source_logs.py
│   ├── registry.py
│   └── utils.py
├── reporting
│   ├── __init__.py
│   ├── accuracy.py
│   ├── confusion_matrix.py
│   ├── macrof1.py
│   ├── microf1.py
│   ├── multi_class_acc.py
│   ├── top_k_svm.py
│   ├── bb_registry.py
│   └── wb_registry.py
├── puLearning							# PULearning third party implementation
│   ├── __init__.py
│   └── puAdapter.py
├── __init__.py
├── LICENSE
├── README.md
├── requirements.txt
├── LogClass.pdf						# Original paper
├── init_params.py						# Parses arguments, initializes global parameters
├── logclass.py							# Performs training and inference of LogClass
├── test_pu.py							# Compares robustness of LogClass
├── train_multi.py						# Trains LogClass for anomalies classification
├── train_binary.py						# Trains LogClass for log anomaly detection
├── run_binary.py						# Loads trained LogClass and detects anomalies
├── decorators.py
└── utils.py</pre>
==== Datasets ====

In this repository we include various [https://github.com/logpai/loghub open-source logs datasets] in the <code>data</code> folder as well as their corresponding preprocessing module in the <code>preprocess</code> package. Additionally there is another preprocessor provided for [https://www.usenix.org/cfdr-data#hpc4 BGL] logs data, which can be downloaded from the link.



=== How to ===

Explain how to use and extend this toolkit.

==== How to add a new dataset ====

Add a new preprocessor module in the <code>preprocess</code> package.

The module should implement a function that follows the <code>preprocess_datset(params)</code> function template included in all preprocessors. It should be decorated with <code>@register(f&quot;{dataset_name}&quot;)</code> , e.g. open''Apache, and call the `process''logs(input''source, output, process''line)<code>function. This</code>process_line` function should also be defined in the processor as well. 

When done, add the module name to the <code>__init__.py</code> list of modules from the <code>preprocess</code> package and also the name from the decorator in the argsparse parameters options as the logs type. For example, <code>--logs_type open_Apache</code>.

===== Preprocessed Logs Format =====

This format is ensured by the <code>process_line</code> function which is to be defined in each preprocessor.

<source lang="python">def process_line(line):
""" Processes a given line from the raw logs 
Parameter
---------
line : str
	One line from the raw logs.

Returns
-------
str
	String with the format f"{label} {msg}" where the `label` indicates whether the log is anomalous and if so, which anomaly category, and `msg` is the filtered log message without parameters.

"""
# your code</source>
To filter the log message parameters, use the <code>remove_parameters(msg)</code>function from the <code>utils.py</code> module in the <code>preprocess</code> package.

==== How to run a new experiment ====

Several experiments examples are included in the repository. The best way to start with creating a new one is to follow the example from the others, specially the main function structure and its experiment function be it training or testing focused.

The key things to consider the experiment should include are the following:

* '''Args parsing''': create custom <code>init_args()</code> and <code>parse_args(args)</code> functions for your experiment that call <code>init_main_args()</code> from the <code>init_params.py</code> module.
* '''Output file handling''': use <code>file_handling(params)</code> function (see <code>utils.py</code> in the main directory of the repo).
* '''Preprocessing raw logs''': if <code>--raw_logs</code> argument is provided, get the preprocessing function using the <code>--logs_type</code> argument from the <code>preprocess</code> module registry calling <code>get_preprocessor(f'{logs_type}')</code> function.
* '''Load logs''': call the <code>load_logs(params, ...)</code> function to get the preprocessed logs from the directory specified in the <code>--logs</code> parameter. It will return a tuple of x, y, and target label names data.

===== Custom experiment =====

Main functions to consider for a custom experiment. Usually in its own function.

'''Feature Engineering'''

* <code>extract_features(x, params)</code> from <code>feature_engineering</code> package's <code>utils.py</code> module: Extracts all specified features in <code>--features</code> parameter from the preprocessed logs. See the function definition for further details.
* <code>build_vocabulary(x)</code> from <code>feature_engineering</code> package's <code>vectorizer.py</code> module: Divides log into tokens and creates vocabulary. See the function definition for further details.
* <code>log_to_vector(x, vocabulary)</code> from <code>feature_engineering</code> package's <code>vectorizer.py</code> module: Vectorizes each log message using a dict of words to index. See the function definition for further details.
* <code>get_features_vector(x_vector, vocabulary, params)</code> from <code>feature_engineering</code> package's <code>utils.py</code> module: Extracts all specified features from the vectorized logs. See the function definition for further details.



'''Model training and inference'''

Each model extends the <code>BaseModel</code> class from module <code>base_model.py</code>. See the class definition for further details.

There are two registries in the <code>models</code> package, one for binary models meant to be used for anomaly detection and another one for multi-classification models to classify the anomalies. Get the constructor for either using the <code>--binary_classifier</code> or <code>--multi_classifier</code> argument specified. E.g. <code>binary_classifier_registry.get_binary_model(params['binary_classifier'])</code>.

By extending <code>BaseModel</code> the model is always saved when it fits the data. Load a model by calling its <code>load()</code> method. It will use the <code>params</code> attribute of the <code>BaseModel</code> class to get the experiment id and load its corresponding model. 

To save the params of an experiment call the <code>save_params(params)</code> function from the <code>utils.py</code> module in the main directory. <code>load_params(params)</code> in case of only using the module for inference. 

'''Reporting'''

There are two kinds of reports, black box and white box and a registry for each in the <code>reporting</code> module.

To use them, call the corresponding registry and obtain the report wrapper using <code>black_box_report_registry.get_bb_report('acc')</code>, for example. 

To add new reports, see the analogous explanation for [[#how-to-add-a-new-model|models]] or [[#how-to-extract-a-new-feature|features]] below.

'''Saving results'''

Among the provided experiments, <code>test_pu.py</code> and <code>train_multi.py</code> save their results creating a dict of column names to lists of results. Then the <code>save_results.py</code> function from the <code>utils.py</code> module is used to save them to a CSV file.



==== How to add a new model ====

To add a new model, implement a class that extends the <code>BaseModel</code> class and include its module in the <code>models</code> package. See the class definition for further details. 

Decorate a method that calls its constructor and returns an instance of the model with the <code>@register(f&quot;{model_name}&quot;)</code>decorator from either the <code>binary_registry.py</code> or the <code>multi_registry.py</code> modules from the <code>models</code> package depending on whether the model is for anomaly detection or classification respectively. 

Finally, make sure you add the module's name in the <code>__init__.py</code> module from the <code>models</code> package and the model option in the <code>init_params.py</code> module within the list for either <code>--binary_classifier</code> or <code>multi_classifier</code> arguments. This way the constructor can be obtained by doing <code>binary_classifier_registry.get_binary_model(params['binary_classifier'])</code>, for example.



==== How to extract a new feature ====

To add a new feature extractor, create a module in the <code>feature_engineering</code> package that wraps your feature extractor function and returns the features. See <code>length.py</code> module as an example for further details.

As in the other cases, decorate the wrapper function with <code>@register(f&quot;{feature_name}&quot;)</code> and make sure you add the module name in the <code>__init__.py</code> from the <code>feature_engineering</code> package and the feature as an option in the <code>init_params.py</code> module <code>--features</code> argument.



=== Included Experiments ===

High level overview of each of the experiments included in the repository. 

==== Testing PULearning ====

<code>test_pu.py</code> is mainly focused on proving the robustness of LogClass for anomaly detection when just providing few labeled data as anomalous.

It would compare PULearning+RandomForest with any other given anomaly detection algorithm. Using the given data, it would start with having only healthy logs on the unlabeled data and gradually increase this up to 10%. To test PULearning, run the following command in the home directory of this project: 

<pre class="">python -m LogClass.test_pu --logs_type &quot;bgl_old&quot; --raw_logs &quot;./Data/RAS from Weibin/RAS_raw_label.dat&quot; --binary_classifier regular --ratio 8 --step 1 --top_percentage 11 --kfold 3</pre>
This would first preprocess the logs. Then, for each kfold iteration, it will perform feature extraction and force a 1:8 ratio of anomalous:healthy logs. Finally with a step of 1% it will go from 0% to 10% anomalous logs in the unlabeled set and compare the accuracy of both anomaly detection algorithms. If none specified it will default to a plain RF. 

==== Testing Anomaly Classification ====

<code>train_multi.py</code> is focused on showing the robustness of LogClass' TF-ILF feature extraction approach for multi-class anomaly classification. The main detail is that when using <code>--kfold N</code>, one can swap training/testing data slices using the <code>--swap</code> flag. This way, for instance, it can train on 10% logs and test on the remaining 90%, when pairing <code>--swap</code> with n ==10. To run such an experiment, use the following command from the parent directory of the project:

<pre class="">python -m LogClass.train_multi --logs_type &quot;open_Apache&quot; --raw_logs &quot;./Data/open_source_logs/&quot; --kfold 10 --swap</pre>
==== Global LogClass ====

<code>logclass.py</code> is set up so that it does both training or testing of the learned models depending on the flags. For example to train and preprocessing run the following command in the home directory of this project: :

<pre class="">python -m LogClass.logclass --train --kfold 3 --logs_type &quot;bgl&quot; --raw_logs &quot;./Data/RAS_LOGS&quot; </pre>
This would first preprocess the raw BGL logs and extract their TF-ILF features, then train and save both PULearning with a RandomForest for anomaly detection and an SVM for multi-class anomaly classification. 

For running inference simply run:

<pre class="">python -m LogClass.logclass --logs_type </pre>
In this case it would load the learned feature extraction approach, both learned models and run inference on the whole logs.

==== Binary training/inference ====

<code>train_binary.py</code> and <code>run_binary.py</code> simply separate the binary part of <code>logclass.py</code> into two modules: one for training both feature extraction and the models, and another one for loading these and running inference.



=== Citing ===

If you find LogClass is useful for your research, please consider citing the paper:

<pre class="">@inproceedings{meng2018device,
  title={Device-agnostic log anomaly classification with partial labels},
  author={Meng, Weibin and Liu, Ying and Zhang, Shenglin and Pei, Dan and Dong, Hui and Song, Lei and Luo, Xulong},
  booktitle={2018 IEEE/ACM 26th International Symposium on Quality of Service (IWQoS)},
  pages={1--6},
  year={2018},
  organization={IEEE}
}</pre>
